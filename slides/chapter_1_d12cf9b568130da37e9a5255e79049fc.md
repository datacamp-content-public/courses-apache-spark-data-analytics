---
title: Insert title here
key: d12cf9b568130da37e9a5255e79049fc

---
## Train/Test Split

```yaml
type: "TitleSlide"
key: "128ea939d1"
```

`@lower_third`

name: Andrew Collier
title: undefined


`@script`
In this lesson we'll be looking at how to split the data into two sets so that we can objectively assess the performance of our model.


---
## Why Split the Data?

```yaml
type: "FullSlide"
key: "9fd5091180"
```

`@part1`
We split the data into two sets so that we can *objectively* evaluate our model.

Typically split in ratio 80:20 giving

- training set (80% of original data) and
- testing set (20% of original data).

Then train model on the training set and evaluate it on the testing set.

**Note:** The split must be *randomised* so that it does not introduce bias.


`@script`
It's important to set aside a portion of the data for testing. You can't use the same data to both train and test a model because this will produce overly optimistic results. Of course the model is going to perform relatively well on the data that it was trained on. We need to test the model on data that it hasn't yet seen.

Typically the data are split into two sets, training and testing, in a 4:1 ratio. This ratio is not set in stone, and a similar proportion will work just as well. The model is then fit to the training set and it's performance is then evaluated on the testing set.

It's important that the split is made at random so that no bias is introduced. However, don't worry, Spark will take care of the randomisation for you.


---
## The Data

```yaml
type: "FullSlide"
key: "593ca7db42"
```

`@part1`
We've already loaded some data as a Spark `DataFrame`.

```
>>> type(power)
pyspark.sql.dataframe.DataFrame
```

We assembled the data into two components: `features` and `power`.

```
>>> power.take(3)
[Row(features=DenseVector([14.96, 41.76, 1024.07, 73.17]), power=463.26),
 Row(features=DenseVector([25.18, 62.96, 1020.04, 59.08]), power=444.37),
 Row(features=DenseVector([5.11, 39.4, 1012.16, 92.14]), power=488.56)]
```


`@script`
Let's take a look at the data that we have already loaded. It's stored in a variable called power which is of type DataFrame. The DataFrame has two columns named features and power, the first of which is an aggregate column assembled from four individual columns. We will ultimately use features to predict power.


---
## Size of Data

```yaml
type: "FullSlide"
key: "8a6cf67fd9"
```

`@part1`
How many records are there in the data?

```
>>> power.count()
9568
```

We need split them them into two sets: train and test.


`@script`
How many records are there in the data? It's easy to check using the count() method. We see that there are around 9500 records, which means that there are plenty to make a reasonable split.


---
## Split the Data

```yaml
type: "FullSlide"
key: "5e0fb8ab96"
```

`@part1`
Use the `randomSplit()` method to split the data.

```
>>> train, test = power.randomSplit([0.8, 0.2], seed=13)
```

Always specify a value for `seed` so that the split is reproducible.

Check on size of `train` and `test`.

```
>>> (train.count(), test.count())
(7608, 1960)
```

Those are the correct proportions of the original data.


`@script`
The actual split is done using the randomSplit() method which accepts two arguments. The first argument is the proportion of records in the split and the second is a random seed. Specifying a random seed is important to ensure reproducibility: you want to get the same results if you run this procedure again!

The results of the split are unpacked immediately into two new variables named train and test. We can check how many records there are in each of them: around 7600 and 2000, which correspond to a 80:20 split of the original 9600 records.


---
## Just One Split?

```yaml
type: "FullSlide"
key: "563931d1c1"
```

`@part1`
Is one split enough?

A single split will only allow you to evaluate your model once.

So you'll have no idea how reliable the performance metric is.

**What are the alternatives?**{{1}}

Cross-validation allows you to do multiple train/test splits.{{2}}

- You can evaluate the model multiple times.{{3}}
- You can gauge how reliable the performance metric is.{{3}}


`@script`
Is it sufficient to just split your data once? Well, a single split is better than no split at all. But just one split means that you only evaluate the model once. Since you just end up with one value for the performance metric there's no way to know how reliable that value is.

But there is a compelling alternative: cross-validation.

With cross-validation you effectively have multiple train/test splits. So you build and evaluate multiple models, giving you multiple values of the performance metric. You can use these to gain a much more robust idea of how well the model actually performs on unseen data.

We'll be looking at cross-validation in more detail in a later section.


---
## Let's try that out!

```yaml
type: "FinalSlide"
key: "ec766569c9"
```

`@script`
The procedure for splitting the data is very simple, but it's critical to ensuring that you evaluate your model objectively.

Let's give it a try!

